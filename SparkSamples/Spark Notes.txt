What is Spark?
RDD?

rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.appName("My first app").getOrCreate()
sc = spark.sparkContext

rdd = sc.parallelize([1,2,3,4,5])
rddData = rdd.collect()

print("No. of Partitions in RDD:", rdd.getNumPartitions())
print("First element:", rdd.first())
print(rddData)


# Create RDD with 6 partitions.
rdd = sc.parallelize([1,2,3,4,5], 6)

emptyRDD = sc.parallelize([])
print("Is RDD empty?:", emptyRDD.isEmpty())

PySpark DataFrames & Pyspark SQL:
DataSet
	distributed collection of data.
	a new interface that has been added from Spark 1.6+
	it gives more added benefits over RDDs with benefits of SPark SQL's optimized execution engine.
		strongly typed
		lambda functions
	can be constructed from JVM objects, perform transformations on that (map, flatMap, filter etc...)
	DataSet API is available only for Scala & Java. It is not available for Python!!!
	
DataFrames (DF)
	DF are a kind of DataSet, but organized into named columns.
	Very similar to tables in a RDBMs.
	But it has a lot of optimization internally.
	DFs can be constructed from various sources:
		data files (structured)
		Hive tables
		RDDs (existing)
		external DBs
		TXT, CSV, JSON, Avro, Parquet, XML (could be from HDFS, S3, Azure Blob storage/filesystem)
	DF APIs are available in Java, Scala, Python and R
	DF is an alias of DataSet[row]
	
	
Creating a DF:

spark = SparkSession.appName("first df").getOrCreate()
sc = spark.sparkContext

data = [("Java", "200000"), ("Python", "100000"), ("Scala", "30000")]
columns = ["language", "users_count"]

rdd = sc.parallelize(data)

dfFromRDD = rdd.toDF()
dfFromRDD.printSchema()

dfFromRDD2 = rdd.toDF(columns)
dfFromRDD2.printSchema()

Customers: 25 columns
Id, Firstname, Lastname, City
SELECT Id, Firstname, Lastname, City
FROM Customers


Create View CustomerView
AS
	SELECT Id, Firstname, Lastname, City
	FROM Customers


SELECT * FROM CustomerView

Customers
Orders
Order_Details
Products

Create View OrderInformation
AS
	SELECT customerinfo, orderinfo, order_details_info, product_info
	FROM Customers
	INNER JOIN Orders ON ....
	INNER JOIN Order_Detailss ON ....
	INNER JOIN Products ON ....

SELECT * FROM OrderInformationView
WHERE Customercity = 'Indore'
GROUP BY
ORDER BY

spark = SparkSession.builder.appName("...").getOrCreate()

df = spark.read.text("people.txt")
df = spark.read.json("people.json")
df = spark.read.csv("people.csv")

Avro, Parquet, Streaming data from Kafka (next week)
ORC - Hive files

Different ways to create empty RDDs:
emptyRDD = sc.parallelize([])
emptyRDD = spark.sparkContext.parallelize([])
emptyRDD = spark.sparkContext.emptyRDD()

schema = StructType([
	StructField("firstname", StringType(), True),
	StructField("middlename", StringType(), True),
	StructField("lastname", StringType(), True),
])

# Create an empty DF from an empty RDD.
# Approach #1
df1 = spark.createDataFrame(emptyRDD)
df1 = spark.createDataFrame(emptyRDD, schema)
df1.printSchema()

# Approach #2
df2 = emptyRDD.toDF()
df2 = emptyRDD.toDF(schema)

# Approach #3
df3 = spark.createDataFrame([], schema)

# Approach #4
df4 = spark.createDataFrame([], StructType([]))

# Show contents
df.show()
df.show(truncate=True)	# Default.
df.show(truncate=False)	
deptDF1.show(truncate=10)
deptDF1.show(truncate=False, n=2)
deptDF1.show(vertical=False)	# Default.
deptDF1.show(vertical=True)


# StructType_StructField variations.
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType
from pyspark.sql.functions import col,struct,when

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

data = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ 
    StructField("firstname",StringType(),True), 
    StructField("middlename",StringType(),True), 
    StructField("lastname",StringType(),True), 
    StructField("id", StringType(), True), 
    StructField("gender", StringType(), True), 
    StructField("salary", IntegerType(), True) 
  ])
 
df = spark.createDataFrame(data=data,schema=schema)
print("Regular schema...")
df.printSchema()
df.show(truncate=False)

# Complex object.
structureData = [
    (("James","","Smith"),"36636","M",3100),
    (("Michael","Rose",""),"40288","M",4300),
    (("Robert","","Williams"),"42114","M",1400),
    (("Maria","Anne","Jones"),"39192","F",5500),
    (("Jen","Mary","Brown"),"","F",-1)
  ]
structureSchema = StructType([
        StructField('name', StructType([
             StructField('firstname', StringType(), True),
             StructField('middlename', StringType(), True),
             StructField('lastname', StringType(), True)
             ])),
         StructField('id', StringType(), True),
         StructField('gender', StringType(), True),
         StructField('salary', IntegerType(), True)
         ])

df2 = spark.createDataFrame(data=structureData,schema=structureSchema)
print("Complex schema...")
df2.printSchema()
df2.show(truncate=False)


updatedDF = df2.withColumn("OtherInfo", 
    struct(col("id").alias("identifier"),
    col("gender").alias("gender"),
    col("salary").alias("salary"),
    when(col("salary").cast(IntegerType()) < 2000,"Low")
      .when(col("salary").cast(IntegerType()) < 4000,"Medium")
      .otherwise("High").alias("Salary_Grade")
  )).drop("id","gender","salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)


# Row class for creating DFs.
pyspark.sql.Row

Before Spark 3.0, a Row class sobject (with named arguments), the fields sorted by name.
Starting with 3.0, a Row class with named arguments, fields are no longer sorted by name. Ordered in the position entered.
To enable sorting by names, set the evn var PYSPARK_ROW_FIELD_SORTING_ENABLED to True

# StructType_StructField.py
import pyspark
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType
from pyspark.sql.functions import col,struct,when

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

data = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ 
    StructField("firstname",StringType(),True), 
    StructField("middlename",StringType(),True), 
    StructField("lastname",StringType(),True), 
    StructField("id", StringType(), True), 
    StructField("gender", StringType(), True), 
    StructField("salary", IntegerType(), True) 
  ])
 
df = spark.createDataFrame(data=data,schema=schema)
print("Regular schema...")
df.printSchema()
df.show(truncate=False)

# Complex object.
structureData = [
    (("James","","Smith"),"36636","M",3100),
    (("Michael","Rose",""),"40288","M",4300),
    (("Robert","","Williams"),"42114","M",1400),
    (("Maria","Anne","Jones"),"39192","F",5500),
    (("Jen","Mary","Brown"),"","F",-1)
  ]
structureSchema = StructType([
        StructField('name', StructType([
             StructField('firstname', StringType(), True),
             StructField('middlename', StringType(), True),
             StructField('lastname', StringType(), True)
             ])),
         StructField('id', StringType(), True),
         StructField('gender', StringType(), True),
         StructField('salary', IntegerType(), True)
         ])

df2 = spark.createDataFrame(data=structureData,schema=structureSchema)
print("Complex schema...")
df2.printSchema()
df2.show(truncate=False)

# # Custom / computed columns. Create columns from existing columns using "withColumn()".
# updatedDF = df2.withColumn("OtherInfo", 
#     struct(col("id").alias("identifier"),
#     col("gender").alias("gender"),
#     col("salary").alias("salary"),
#     when(col("salary").cast(IntegerType()) < 2000,"Low")
#       .when(col("salary").cast(IntegerType()) < 4000,"Medium")
#       .otherwise("High").alias("Salary_Grade")
#   )).drop("id","gender","salary")

# updatedDF.printSchema()
# updatedDF.show(truncate=False)


# # """ Array & Map"""
# # arrayStructureSchema = StructType([
# #     StructField('name', StructType([
# #        StructField('firstname', StringType(), True),
# #        StructField('middlename', StringType(), True),
# #        StructField('lastname', StringType(), True)
# #        ])),
# #        StructField('hobbies', ArrayType(StringType()), True),
# #        StructField('properties', MapType(StringType(),StringType()), True)
# #     ])

# # ArrayType example:
# dataArr = [
#  ("James,,Smith",["Java","Scala","C++"],["Spark","Java"],"OH","CA"),
#  ("Michael,Rose,",["Spark","Java","C++"],["Spark","Java"],"NY","NJ"),
#  ("Robert,,Williams",["CSharp","VB"],["Spark","Python"],"UT","NV")
# ]

# schemaArr = StructType([ 
#     StructField("name",StringType(),True), 
#     StructField("languagesAtSchool",ArrayType(StringType()),True), 
#     StructField("languagesAtWork",ArrayType(StringType()),True), 
#     StructField("currentState", StringType(), True), 
#     StructField("previousState", StringType(), True)
#   ])

# dfArr = spark.createDataFrame(data=dataArr,schema=schemaArr)
# print("ArrayType example...")
# dfArr.printSchema()
# dfArr.show()

# # MapType example:
# schemaMap = StructType([
#     StructField('name', StringType(), True),
#     StructField('properties', MapType(StringType(),StringType()),True)
# ])

# dataDictionary = [
#         ('James',{'hair':'black','eye':'brown'}),
#         ('Michael',{'hair':'brown','eye':None}),
#         ('Robert',{'hair':'red','eye':'black'}),
#         ('Washington',{'hair':'grey','eye':'grey'}),
#         ('Jefferson',{'hair':'brown','eye':''})
#         ]
# dfMap = spark.createDataFrame(data=dataDictionary, schema = schemaMap)
# print("MapType example...")
# dfMap.printSchema()
# dfMap.show(truncate=False)

# # Creating StructType object struct from JSON file.
# # Print the schema in JSON format.
# print("Schema in json format...")
# print(df2.schema.json())

# # This will return an relatively simpler schema format.
# print("Schema in simple string format...")
# print(df2.schema.simpleString())

# # Use the JSON to create a DF.
# print("Create DF using a JSON Schema....")
# import json
# schema_dict = {"fields":[{"metadata":{},"name":"name","nullable":True,"type":{"fields":[{"metadata":{},"name":"firstname","nullable":True,"type":"string"},{"metadata":{},"name":"middlename","nullable":True,"type":"string"},{"metadata":{},"name":"lastname","nullable":True,"type":"string"}],"type":"struct"}},{"metadata":{},"name":"id","nullable":True,"type":"string"},{"metadata":{},"name":"gender","nullable":True,"type":"string"},{"metadata":{},"name":"salary","nullable":True,"type":"integer"}],"type":"struct"}

# #schemaFromJson = StructType.fromJson(json.loads("resources/spark_examples/schema.json"))
# schemaFromJson = StructType.fromJson(schema_dict)
# df3 = spark.createDataFrame(
#         spark.sparkContext.parallelize(structureData),schemaFromJson)
# df3.printSchema()
# df3.show()

# Create DF using Row.
print("Create DF using Row...")
print("Access values using index...")
row = Row("James", 40)
print(row[0], row[1])

print("Access values using field names (named arguments)...")
row2 = Row(name = "Mary", age = 2)
print(row2.name, row2.age)

print("Define a custom class from Row...")
Person = Row("name", "age")
p1 = Person("John", 25)
p2 = Person("Mary", 21)
print(p1.name, p1.age)
print(p2.name, p2.age)

# Use Row class on RDD.
dataRow = [Row(name="James,,Smith",lang=["Java","Scala","C++"], state="OH"),
  Row(name="Michael,Rose,",lang=["Spark","Java","C++"],state="NY"),
  Row(name="Robert,,Williams",lang=["CSharp","VB"],state="UT")
]

rddRows = spark.sparkContext.parallelize(dataRow)
print("Printing rdd created using Rows...")
print(rddRows.collect())

rddRowsCollected = rddRows.collect()
print("Printing rdd created using Rows using a 'for' loop...")
for row in rddRowsCollected:
  print(row.name, row.lang, row.state)

# Create DF using custom Row class.
Person = Row("name", "lang", "state")
dataRow = [Person("James,,Smith",["Java","Scala","C++"], "OH"),
  Person("Michael,Rose,",["Spark","Java","C++"],"NY"),
  Person("Robert,,Williams",["CSharp","VB"],"UT")
]

dfRows = spark.createDataFrame(dataRow)
print("DF created using custom Row class...")
df.printSchema()
df.show()

columns = ["name", "languagesUsed", "currentState"]
dfRowsWithColumnList = spark.createDataFrame(dataRow).toDF(*columns)
print("Change column name(s) using .toDF()...")
dfRowsWithColumnList.printSchema()
dfRowsWithColumnList.show()

# Nested Rows.
dataNestedRows = [Row(name="John", properties=Row(hair="brown", eye="black")),
  Row(name="Mary", properties=Row(hair="blonde", eye="green")),
  Row(name="Joe", properties=Row(hair="black", eye="blue"))
]
dfNestedRows = spark.createDataFrame(dataNestedRows)
print("DF with nested rows...")
dfNestedRows.printSchema()
dfNestedRows.show()



