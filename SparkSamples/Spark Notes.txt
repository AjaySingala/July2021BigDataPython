What is Spark?
RDD?

rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])

import pyspark
from pyspark.sql import SparkSession

spark = SparkSession.appName("My first app").getOrCreate()
sc = spark.sparkContext

rdd = sc.parallelize([1,2,3,4,5])
rddData = rdd.collect()

print("No. of Partitions in RDD:", rdd.getNumPartitions())
print("First element:", rdd.first())
print(rddData)


# Create RDD with 6 partitions.
rdd = sc.parallelize([1,2,3,4,5], 6)

emptyRDD = sc.parallelize([])
print("Is RDD empty?:", emptyRDD.isEmpty())

PySpark DataFrames & Pyspark SQL:
DataSet
	distributed collection of data.
	a new interface that has been added from Spark 1.6+
	it gives more added benefits over RDDs with benefits of SPark SQL's optimized execution engine.
		strongly typed
		lambda functions
	can be constructed from JVM objects, perform transformations on that (map, flatMap, filter etc...)
	DataSet API is available only for Scala & Java. It is not available for Python!!!
	
DataFrames (DF)
	DF are a kind of DataSet, but organized into named columns.
	Very similar to tables in a RDBMs.
	But it has a lot of optimization internally.
	DFs can be constructed from various sources:
		data files (structured)
		Hive tables
		RDDs (existing)
		external DBs
		TXT, CSV, JSON, Avro, Parquet, XML (could be from HDFS, S3, Azure Blob storage/filesystem)
	DF APIs are available in Java, Scala, Python and R
	DF is an alias of DataSet[row]
	
	
Creating a DF:

spark = SparkSession.appName("first df").getOrCreate()
sc = spark.sparkContext

data = [("Java", "200000"), ("Python", "100000"), ("Scala", "30000")]
columns = ["language", "users_count"]

rdd = sc.parallelize(data)

dfFromRDD = rdd.toDF()
dfFromRDD.printSchema()

dfFromRDD2 = rdd.toDF(columns)
dfFromRDD2.printSchema()

Customers: 25 columns
Id, Firstname, Lastname, City
SELECT Id, Firstname, Lastname, City
FROM Customers


Create View CustomerView
AS
	SELECT Id, Firstname, Lastname, City
	FROM Customers


SELECT * FROM CustomerView

Customers
Orders
Order_Details
Products

Create View OrderInformation
AS
	SELECT customerinfo, orderinfo, order_details_info, product_info
	FROM Customers
	INNER JOIN Orders ON ....
	INNER JOIN Order_Detailss ON ....
	INNER JOIN Products ON ....

SELECT * FROM OrderInformationView
WHERE Customercity = 'Indore'
GROUP BY
ORDER BY

spark = SparkSession.builder.appName("...").getOrCreate()

df = spark.read.text("people.txt")
df = spark.read.json("people.json")
df = spark.read.csv("people.csv")

Avro, Parquet, Streaming data from Kafka (next week)
ORC - Hive files

Different ways to create empty RDDs:
emptyRDD = sc.parallelize([])
emptyRDD = spark.sparkContext.parallelize([])
emptyRDD = spark.sparkContext.emptyRDD()

schema = StructType([
	StructField("firstname", StringType(), True),
	StructField("middlename", StringType(), True),
	StructField("lastname", StringType(), True),
])

# Create an empty DF from an empty RDD.
# Approach #1
df1 = spark.createDataFrame(emptyRDD)
df1 = spark.createDataFrame(emptyRDD, schema)
df1.printSchema()

# Approach #2
df2 = emptyRDD.toDF()
df2 = emptyRDD.toDF(schema)

# Approach #3
df3 = spark.createDataFrame([], schema)

# Approach #4
df4 = spark.createDataFrame([], StructType([]))

# Show contents
df.show()
df.show(truncate=True)	# Default.
df.show(truncate=False)	
deptDF1.show(truncate=10)
deptDF1.show(truncate=False, n=2)
deptDF1.show(vertical=False)	# Default.
deptDF1.show(vertical=True)


# StructType_StructField variations.
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType
from pyspark.sql.functions import col,struct,when

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

data = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ 
    StructField("firstname",StringType(),True), 
    StructField("middlename",StringType(),True), 
    StructField("lastname",StringType(),True), 
    StructField("id", StringType(), True), 
    StructField("gender", StringType(), True), 
    StructField("salary", IntegerType(), True) 
  ])
 
df = spark.createDataFrame(data=data,schema=schema)
print("Regular schema...")
df.printSchema()
df.show(truncate=False)

# Complex object.
structureData = [
    (("James","","Smith"),"36636","M",3100),
    (("Michael","Rose",""),"40288","M",4300),
    (("Robert","","Williams"),"42114","M",1400),
    (("Maria","Anne","Jones"),"39192","F",5500),
    (("Jen","Mary","Brown"),"","F",-1)
  ]
structureSchema = StructType([
        StructField('name', StructType([
             StructField('firstname', StringType(), True),
             StructField('middlename', StringType(), True),
             StructField('lastname', StringType(), True)
             ])),
         StructField('id', StringType(), True),
         StructField('gender', StringType(), True),
         StructField('salary', IntegerType(), True)
         ])

df2 = spark.createDataFrame(data=structureData,schema=structureSchema)
print("Complex schema...")
df2.printSchema()
df2.show(truncate=False)


updatedDF = df2.withColumn("OtherInfo", 
    struct(col("id").alias("identifier"),
    col("gender").alias("gender"),
    col("salary").alias("salary"),
    when(col("salary").cast(IntegerType()) < 2000,"Low")
      .when(col("salary").cast(IntegerType()) < 4000,"Medium")
      .otherwise("High").alias("Salary_Grade")
  )).drop("id","gender","salary")

updatedDF.printSchema()
updatedDF.show(truncate=False)


# Row class for creating DFs.
pyspark.sql.Row

Before Spark 3.0, a Row class sobject (with named arguments), the fields sorted by name.
Starting with 3.0, a Row class with named arguments, fields are no longer sorted by name. Ordered in the position entered.
To enable sorting by names, set the evn var PYSPARK_ROW_FIELD_SORTING_ENABLED to True

# StructType_StructField.py
import pyspark
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType
from pyspark.sql.functions import col,struct,when

spark = SparkSession.builder.master("local[1]") \
                    .appName('SparkByExamples.com') \
                    .getOrCreate()

data = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ 
    StructField("firstname",StringType(),True), 
    StructField("middlename",StringType(),True), 
    StructField("lastname",StringType(),True), 
    StructField("id", StringType(), True), 
    StructField("gender", StringType(), True), 
    StructField("salary", IntegerType(), True) 
  ])
 
df = spark.createDataFrame(data=data,schema=schema)
print("Regular schema...")
df.printSchema()
df.show(truncate=False)

# Complex object.
structureData = [
    (("James","","Smith"),"36636","M",3100),
    (("Michael","Rose",""),"40288","M",4300),
    (("Robert","","Williams"),"42114","M",1400),
    (("Maria","Anne","Jones"),"39192","F",5500),
    (("Jen","Mary","Brown"),"","F",-1)
  ]
structureSchema = StructType([
        StructField('name', StructType([
             StructField('firstname', StringType(), True),
             StructField('middlename', StringType(), True),
             StructField('lastname', StringType(), True)
             ])),
         StructField('id', StringType(), True),
         StructField('gender', StringType(), True),
         StructField('salary', IntegerType(), True)
         ])

df2 = spark.createDataFrame(data=structureData,schema=structureSchema)
print("Complex schema...")
df2.printSchema()
df2.show(truncate=False)

# # Custom / computed columns. Create columns from existing columns using "withColumn()".
# updatedDF = df2.withColumn("OtherInfo", 
#     struct(col("id").alias("identifier"),
#     col("gender").alias("gender"),
#     col("salary").alias("salary"),
#     when(col("salary").cast(IntegerType()) < 2000,"Low")
#       .when(col("salary").cast(IntegerType()) < 4000,"Medium")
#       .otherwise("High").alias("Salary_Grade")
#   )).drop("id","gender","salary")

# updatedDF.printSchema()
# updatedDF.show(truncate=False)


# # """ Array & Map"""
# # arrayStructureSchema = StructType([
# #     StructField('name', StructType([
# #        StructField('firstname', StringType(), True),
# #        StructField('middlename', StringType(), True),
# #        StructField('lastname', StringType(), True)
# #        ])),
# #        StructField('hobbies', ArrayType(StringType()), True),
# #        StructField('properties', MapType(StringType(),StringType()), True)
# #     ])

# # ArrayType example:
# dataArr = [
#  ("James,,Smith",["Java","Scala","C++"],["Spark","Java"],"OH","CA"),
#  ("Michael,Rose,",["Spark","Java","C++"],["Spark","Java"],"NY","NJ"),
#  ("Robert,,Williams",["CSharp","VB"],["Spark","Python"],"UT","NV")
# ]

# schemaArr = StructType([ 
#     StructField("name",StringType(),True), 
#     StructField("languagesAtSchool",ArrayType(StringType()),True), 
#     StructField("languagesAtWork",ArrayType(StringType()),True), 
#     StructField("currentState", StringType(), True), 
#     StructField("previousState", StringType(), True)
#   ])

# dfArr = spark.createDataFrame(data=dataArr,schema=schemaArr)
# print("ArrayType example...")
# dfArr.printSchema()
# dfArr.show()

# # MapType example:
# schemaMap = StructType([
#     StructField('name', StringType(), True),
#     StructField('properties', MapType(StringType(),StringType()),True)
# ])

# dataDictionary = [
#         ('James',{'hair':'black','eye':'brown'}),
#         ('Michael',{'hair':'brown','eye':None}),
#         ('Robert',{'hair':'red','eye':'black'}),
#         ('Washington',{'hair':'grey','eye':'grey'}),
#         ('Jefferson',{'hair':'brown','eye':''})
#         ]
# dfMap = spark.createDataFrame(data=dataDictionary, schema = schemaMap)
# print("MapType example...")
# dfMap.printSchema()
# dfMap.show(truncate=False)

# # Creating StructType object struct from JSON file.
# # Print the schema in JSON format.
# print("Schema in json format...")
# print(df2.schema.json())

# # This will return an relatively simpler schema format.
# print("Schema in simple string format...")
# print(df2.schema.simpleString())

# # Use the JSON to create a DF.
# print("Create DF using a JSON Schema....")
# import json
# schema_dict = {"fields":[{"metadata":{},"name":"name","nullable":True,"type":{"fields":[{"metadata":{},"name":"firstname","nullable":True,"type":"string"},{"metadata":{},"name":"middlename","nullable":True,"type":"string"},{"metadata":{},"name":"lastname","nullable":True,"type":"string"}],"type":"struct"}},{"metadata":{},"name":"id","nullable":True,"type":"string"},{"metadata":{},"name":"gender","nullable":True,"type":"string"},{"metadata":{},"name":"salary","nullable":True,"type":"integer"}],"type":"struct"}

# #schemaFromJson = StructType.fromJson(json.loads("resources/spark_examples/schema.json"))
# schemaFromJson = StructType.fromJson(schema_dict)
# df3 = spark.createDataFrame(
#         spark.sparkContext.parallelize(structureData),schemaFromJson)
# df3.printSchema()
# df3.show()

# Create DF using Row.
print("Create DF using Row...")
print("Access values using index...")
row = Row("James", 40)
print(row[0], row[1])

print("Access values using field names (named arguments)...")
row2 = Row(name = "Mary", age = 2)
print(row2.name, row2.age)

print("Define a custom class from Row...")
Person = Row("name", "age")
p1 = Person("John", 25)
p2 = Person("Mary", 21)
print(p1.name, p1.age)
print(p2.name, p2.age)

# Use Row class on RDD.
dataRow = [Row(name="James,,Smith",lang=["Java","Scala","C++"], state="OH"),
  Row(name="Michael,Rose,",lang=["Spark","Java","C++"],state="NY"),
  Row(name="Robert,,Williams",lang=["CSharp","VB"],state="UT")
]

rddRows = spark.sparkContext.parallelize(dataRow)
print("Printing rdd created using Rows...")
print(rddRows.collect())

rddRowsCollected = rddRows.collect()
print("Printing rdd created using Rows using a 'for' loop...")
for row in rddRowsCollected:
  print(row.name, row.lang, row.state)

# Create DF using custom Row class.
Person = Row("name", "lang", "state")
dataRow = [Person("James,,Smith",["Java","Scala","C++"], "OH"),
  Person("Michael,Rose,",["Spark","Java","C++"],"NY"),
  Person("Robert,,Williams",["CSharp","VB"],"UT")
]

dfRows = spark.createDataFrame(dataRow)
print("DF created using custom Row class...")
df.printSchema()
df.show()

columns = ["name", "languagesUsed", "currentState"]
dfRowsWithColumnList = spark.createDataFrame(dataRow).toDF(*columns)
print("Change column name(s) using .toDF()...")
dfRowsWithColumnList.printSchema()
dfRowsWithColumnList.show()

# Nested Rows.
dataNestedRows = [Row(name="John", properties=Row(hair="brown", eye="black")),
  Row(name="Mary", properties=Row(hair="blonde", eye="green")),
  Row(name="Joe", properties=Row(hair="black", eye="blue"))
]
dfNestedRows = spark.createDataFrame(dataNestedRows)
print("DF with nested rows...")
dfNestedRows.printSchema()
dfNestedRows.show()


# Column class.
pyspark.sql.Column
several functions to work on DFs to manipulate column values, evaluation expressions (boolean), filter rows, retrieve part of a value from a DF column, list, map, struct cols.

lit() => short literal

stratsWith()
endsWith()
isNull()
isNotNull()
substr()
isIn()	= check if a value exists in a list
getField()
	name="John", prop=(hair="black",eye="brown")
	df.prop.getField("hair")
getItem()
	getItem(2)
	
df.select("fname")
df.select("fname", "lname")
df.select(df.fname, df.lname)
df.select(df["fname"], df["lname"])
df.select(col("fname"), col("lname"))
df.select(df.colRegex("`^.*name*`"]).show()
df.select("*").show()
df.select([col for col in df.columns()]).show()
df.select(df.columns[:3]).show()
df.select(df.columns[2:4]).show(3)


pyspark_column_class.py
import pyspark
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType
from pyspark.sql.functions import col,struct,when, lit

spark = SparkSession.builder.master("local[1]") \
                    .appName('ajaysingala.com') \
                    .getOrCreate()

# simplests way to create a column.
colObj = lit("ajaysingala.com")

data = [("John", 23), ("Mary", 21)]
df = spark.createDataFrame(data).toDF("name.firstname", "age")
df.printSchema()
df.show()

df.select(df.age).show()
df.select(df["age"]).show()
# Access column names with dot and backticks (`).
df.select(df["`name.firstname`"]).show()

# Using SQL col().
print("print using col()...")
df.select(col("gender")).show()
# Access column names with dot and backticks (`).
df.select(col("`name.firstname`")).show()

# Access struct type columns.
data2 = [Row(name="John", prop=Row(hair="black", eye="blue")),
    Row(name="Mary", prop=Row(hair="blonde", eye="green")),
    Row(name="John", prop=Row(hair="brown", eye="black"))
]
df2 = spark.createDataFrame(data2)
print("Schema...")
df2.printSchema()
df2.show()

print("Access struct cols...")
df2.select(df.prop.hair).show()
df2.select(df["prop.hair"]).show()
df2.select(col("prop.hair")).show()

print("Access all strut cols...")
df2.select(col("prop.*")).show()

# Column Operators.
data3 = [(100,2,1), (200,3,4), (300,4,4)]
df = spark.createDataFrame(data3).toDF("col1", "col2", "col3")

# Arithmetic operations.
print("column arithmetic operations...")
df.select(df.col1 + df.col2).show()
df.select(df.col1 - df.col2).show()
df.select(df.col1 * df.col2).show()
df.select(df.col1 / df.col2).show()
df.select(df.col1 % df.col2).show()

df.select(df.col2 > df.col3).show()
df.select(df.col2 < df.col3).show()
df.select(df.col2 == df.col3).show()

# Column functions.
data4 = [("James","Bond","100",None),
      ("Ann","Varsa","200",'F'),
      ("Tom Cruise","XXX","400",''),
      ("Tom Brand",None,"400",'M')] 
columns4 = ["fname","lname","id","gender"]
df4=spark.createDataFrame(data4,columns4)

# alias().
from pyspark.sql.functions import expr
print("alias()...")
df4.select(df4.fname.alias("first_name"), \
          df4.lname.alias("last_name")
   ).show()

#Another example
df4.select(expr(" lname ||', '|| fname").alias("fullName") \
   ).show()

# sort(), .asc() and .desc().
print("asc() and desc()...")
df4.sort(df4.fname.asc()).show()
df4.sort(df4.fname.desc()).show()

# between().
print("between()...")
df4.filter(df4.id.between(100,300)).show()

# contains().
print("contains()...")
df4.filter(df4.fname.contains("Cruise")).show()

# like().
print("like()...")
df4.select(df4.fname,df.lname,df.id) \
  .filter(df4.fname.like("%om")) 

# when & otherwise
from pyspark.sql.functions import when
df4.select(df4.fname,df.lname,when(df4.gender=="M","Male") \
	.when(df4.gender=="F","Female") \
	.when(col("gender").isNull() ,"undefined") \
	.otherwise(df4.gender).alias("new_gender") \
).show()

# pyspark_withColumn.py
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from pyspark.sql.types import StructType, StructField, StringType,IntegerType

spark = SparkSession.builder.appName('ajaysingala.com').getOrCreate()

data = [('James','','Smith','1991-04-01','M',3000),
  ('Michael','Rose','','2000-05-19','M',4000),
  ('Robert','','Williams','1978-09-05','M',4000),
  ('Maria','Anne','Jones','1967-12-01','F',4000),
  ('Jen','Mary','Brown','1980-02-17','F',-1)
]

columns = ["firstname","middlename","lastname","dob","gender","salary"]
df = spark.createDataFrame(data=data, schema = columns)
print("The original schema...")
df.printSchema()
df.show(truncate=False)

df2 = df.withColumn("salary",col("salary").cast("Integer"))
print("df2 with salary as integer...")
df2.printSchema()
df2.show(truncate=False)

df3 = df.withColumn("salary",col("salary")*100)
print("salary * 100...")
df3.printSchema()
df3.show(truncate=False) 

df4 = df.withColumn("CopiedColumn",col("salary")* -1)
print("Copy the column 'salary'...")
df4.printSchema()
df4.show()

df5 = df.withColumn("Country", lit("USA"))
print("Literal value to column 'Country'...")
df5.printSchema()
df5.show()

df6 = df.withColumn("Country", lit("USA")) \
   .withColumn("anotherColumn",lit("anotherValue"))
print("Literal value to column 'Country' and create another column with a literal value...")
df6.printSchema()
df6.show()

df.withColumnRenamed("gender","sex") \
  .show(truncate=False) 
print("Rename column 'gender' to 'sex' in the original DF...")
df.printSchema()
df.show()

df4.drop("CopiedColumn") \
.show(truncate=False) 
print("Drop the column 'CopiedColumn from df4...")
df4.printSchema()
df4.show()
